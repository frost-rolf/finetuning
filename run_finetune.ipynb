{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tune LLaMA 3.1-8B Instruct Model on Custom Dataset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch transformers datasets accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
                "\n",
                "# Load the LLaMA 3.1-8B Instruct model and tokenizer\n",
                "model_name = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
                "\n",
                "# Load the dataset\n",
                "dataset = load_dataset('json', data_files='./data/combined_dataset_for_finetuning.json')\n",
                "\n",
                "# Tokenize the dataset\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
                "\n",
                "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
                "\n",
                "# Define training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir='./results',\n",
                "    evaluation_strategy='epoch',\n",
                "    per_device_train_batch_size=1,\n",
                "    per_device_eval_batch_size=1,\n",
                "    num_train_epochs=3,\n",
                "    save_strategy='epoch',\n",
                "    logging_dir='./logs',\n",
                "    logging_steps=10,\n",
                "    fp16=True,  # Mixed precision training\n",
                ")\n",
                "\n",
                "# Initialize the Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset['train'],\n",
                "    eval_dataset=tokenized_dataset['validation'],\n",
                ")\n",
                "\n",
                "# Start fine-tuning\n",
                "trainer.train()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
